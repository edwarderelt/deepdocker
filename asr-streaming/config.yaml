base_image:
  image: vllm/vllm-openai:latest

docker_server:
  start_command: sh -c "HF_TOKEN=$(cat /secrets/hf_access_token) vllm serve meta-llama/Meta-Llama-3.1-8B-Instruct --port 8000 --max-model-len 1024"
  readiness_endpoint: /health
  liveness_endpoint: /health
  predict_endpoint: /v1/chat/completions
  server_port: 8000

resources:
  accelerator: A10G

model_name: vllm-model-server

secrets:
  hf_access_token: null

runtime:
  predict_concurrency: 128